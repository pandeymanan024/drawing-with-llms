{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11591735,"sourceType":"datasetVersion","datasetId":7268816},{"sourceId":224423433,"sourceType":"kernelVersion"},{"sourceId":226192131,"sourceType":"kernelVersion"},{"sourceId":226327421,"sourceType":"kernelVersion"},{"sourceId":227813505,"sourceType":"kernelVersion"},{"sourceId":227813508,"sourceType":"kernelVersion"},{"sourceId":4527,"sourceType":"modelInstanceVersion","modelInstanceId":3319,"modelId":971},{"sourceId":263093,"sourceType":"modelInstanceVersion","modelInstanceId":225001,"modelId":164716}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:44:29.950748Z","iopub.execute_input":"2025-04-27T19:44:29.950959Z","iopub.status.idle":"2025-04-27T19:44:32.166910Z","shell.execute_reply.started":"2025-04-27T19:44:29.950942Z","shell.execute_reply":"2025-04-27T19:44:32.166041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install cairosvg\n!pip install clip\n!pip install --upgrade bitsAndBytes\n!pip install --upgrade openai-clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:44:38.553150Z","iopub.execute_input":"2025-04-27T19:44:38.553385Z","iopub.status.idle":"2025-04-27T19:46:14.371797Z","shell.execute_reply.started":"2025-04-27T19:44:38.553368Z","shell.execute_reply":"2025-04-27T19:46:14.371049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\nimport kagglehub\n\nimport os\nimport io\nimport re\nimport random\nimport base64\nfrom io import BytesIO\n\nimport time\nfrom datetime import timedelta\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.functional as F\n\nfrom IPython.display import SVG\n\nfrom PIL import Image\nimport cv2\n\nfrom diffusers import StableDiffusionPipeline\nfrom transformers import AutoProcessor, AutoModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:51:55.143192Z","iopub.execute_input":"2025-04-27T19:51:55.144035Z","iopub.status.idle":"2025-04-27T19:51:55.148722Z","shell.execute_reply.started":"2025-04-27T19:51:55.144008Z","shell.execute_reply":"2025-04-27T19:51:55.148010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\nimport io\nfrom math import prod\nfrom statistics import mean\n\nfrom IPython.display import SVG\n\nimport cairosvg\nimport clip\nimport kagglehub\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom transformers import (\n    AutoProcessor,\n    BitsAndBytesConfig,\n    PaliGemmaForConditionalGeneration,\n)\n\nsvg_constraints = kagglehub.package_import('metric/svg-constraints')\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str\n) -> float:\n    \"\"\"Calculates a fidelity score by comparing generated SVG images to target text descriptions.\n\n    Parameters\n    ----------\n    solution : pd.DataFrame\n        A DataFrame containing target text descriptions. Must have a column named 'description'.\n    submission : pd.DataFrame\n        A DataFrame containing generated SVG strings. Must have a column named 'svg'.\n    row_id_column_name : str\n        The name of the column containing row identifiers. This column is removed before scoring.\n\n    Returns\n    -------\n    float\n        The mean fidelity score (a value between 0 and 1) representing the average similarity between the generated SVGs and their descriptions.\n        A higher score indicates better fidelity.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the 'svg' column in the submission DataFrame is not of string type or if validation of the SVG fails.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'description': ['red ball', 'swimming pool']\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'svg': ['<svg viewBox=\"0 0 100 100\"><circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"red\"/></svg>',\n    ...         '<svg viewBox=\"0 0 100 100\"><rect x=\"10\" y=\"10\" width=\"80\" height=\"80\" fill=\"blue\"/></svg>']\n    ... })\n    >>> score(solution, submission, 'id')\n    0...\n    \"\"\"\n    # Validate\n    del solution[row_id_column_name], submission[row_id_column_name]\n    if not pd.api.types.is_string_dtype(submission.loc[:, 'svg']):\n        raise ParticipantVisibleError('svg must be a string.')\n    # check that SVG code meets defined constraints\n    constraints = svg_constraints.SVGConstraints()\n    try:\n        for svg in submission.loc[:, 'svg']:\n            constraints.validate_svg(svg)\n    except:\n        raise ParticipantVisibleError('SVG code violates constraints.')\n\n    # Score\n    vqa_evaluator = VQAEvaluator()\n    aesthetic_evaluator = AestheticEvaluator()\n\n    results = []\n    try:\n        for svg, description in zip(\n            submission.loc[:, 'svg'], solution.loc[:, 'description'], strict=True\n        ):\n            image = svg_to_png(svg)\n            vqa_score = vqa_evaluator.score(image, 'SVG illustration of ' + description)\n            aesthetic_score = aesthetic_evaluator.score(image)\n            instance_score = harmonic_mean(vqa_score, aesthetic_score, beta=2.0)\n            results.append(instance_score)\n\n    except:\n        raise ParticipantVisibleError('SVG failed to score.')\n\n    fidelity = mean(results)\n    return float(fidelity)\n\n\nclass VQAEvaluator:\n    \"\"\"Evaluates images based on their similarity to a given text description.\"\"\"\n\n    def __init__(self):\n        self.quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.float16,\n        )\n        self.model_path = kagglehub.model_download(\n            'google/paligemma-2/transformers/paligemma2-10b-mix-448'\n        )\n        self.processor = AutoProcessor.from_pretrained(self.model_path)\n        self.model = PaliGemmaForConditionalGeneration.from_pretrained(\n            self.model_path,\n            low_cpu_mem_usage=True,\n            quantization_config=self.quantization_config,\n        )\n        self.questions = {\n            'fidelity': 'Does <image> portray \"{}\" without any lettering? Answer yes or no.',\n            'text': '<image> Text present: yes or no?',\n        }\n\n    def score(self, image: Image.Image, description: str) -> float:\n        \"\"\"Evaluates the fidelity of an image to a target description using VQA yes/no probabilities.\n\n        Parameters\n        ----------\n        image : PIL.Image.Image\n            The image to evaluate.\n        description : str\n            The text description that the image should represent.\n\n        Returns\n        -------\n        float\n            The score (a value between 0 and 1) representing the match between the image and its description.\n        \"\"\"\n        p_fidelity = self.get_yes_probability(image, self.questions['fidelity'].format(description))\n        p_text = self.get_yes_probability(image, self.questions['text'])\n        return p_fidelity * (1 - p_text)\n\n    def mask_yes_no(self, logits):\n        \"\"\"Masks logits for 'yes' or 'no'.\"\"\"\n        yes_token_id = self.processor.tokenizer.convert_tokens_to_ids('yes')\n        no_token_id = self.processor.tokenizer.convert_tokens_to_ids('no')\n        yes_with_space_token_id = self.processor.tokenizer.convert_tokens_to_ids(' yes')\n        no_with_space_token_id = self.processor.tokenizer.convert_tokens_to_ids(' no')\n\n        mask = torch.full_like(logits, float('-inf'))\n        mask[:, yes_token_id] = logits[:, yes_token_id]\n        mask[:, no_token_id] = logits[:, no_token_id]\n        mask[:, yes_with_space_token_id] = logits[:, yes_with_space_token_id]\n        mask[:, no_with_space_token_id] = logits[:, no_with_space_token_id]\n        return mask\n\n    def get_yes_probability(self, image, prompt) -> float:\n        inputs = self.processor(images=image, text=prompt, return_tensors='pt').to(\n            'cuda:0'\n        )\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits = outputs.logits[:, -1, :]  # Logits for the last (predicted) token\n            masked_logits = self.mask_yes_no(logits)\n            probabilities = torch.softmax(masked_logits, dim=-1)\n\n        yes_token_id = self.processor.tokenizer.convert_tokens_to_ids('yes')\n        no_token_id = self.processor.tokenizer.convert_tokens_to_ids('no')\n        yes_with_space_token_id = self.processor.tokenizer.convert_tokens_to_ids(' yes')\n        no_with_space_token_id = self.processor.tokenizer.convert_tokens_to_ids(' no')\n\n        prob_yes = probabilities[0, yes_token_id].item()\n        prob_no = probabilities[0, no_token_id].item()\n        prob_yes_space = probabilities[0, yes_with_space_token_id].item()\n        prob_no_space = probabilities[0, no_with_space_token_id].item()\n\n        total_yes_prob = prob_yes + prob_yes_space\n        total_no_prob = prob_no + prob_no_space\n\n        total_prob = total_yes_prob + total_no_prob\n        renormalized_yes_prob = total_yes_prob / total_prob\n\n        return renormalized_yes_prob\n\n\nclass AestheticPredictor(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.input_size = input_size\n        self.layers = nn.Sequential(\n            nn.Linear(self.input_size, 1024),\n            nn.Dropout(0.2),\n            nn.Linear(1024, 128),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.Dropout(0.1),\n            nn.Linear(64, 16),\n            nn.Linear(16, 1),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass AestheticEvaluator:\n    def __init__(self):\n\n# -----------------modified paths for packaging!!!!-----------------\n        self.model_path = kagglehub.notebook_output_download(\n            'metric/sac-logos-ava1-l14-linearmse'\n        ) + '/sac+logos+ava1-l14-linearMSE.pth'\n\n        self.clip_model_path = kagglehub.notebook_output_download(\n            'metric/openai-clip-vit-large-patch14'\n        ) + '/ViT-L-14.pt'\n\n        self.predictor, self.clip_model, self.preprocessor = self.load()\n\n    def load(self):\n        \"\"\"Loads the aesthetic predictor model and CLIP model.\"\"\"\n        state_dict = torch.load(self.model_path, weights_only=True, map_location='cuda:1')\n\n        # CLIP embedding dim is 768 for CLIP ViT L 14\n        predictor = AestheticPredictor(768)\n        predictor.load_state_dict(state_dict)\n        predictor.to('cuda:1')\n        predictor.eval()\n        clip_model, preprocessor = clip.load(self.clip_model_path, device='cuda:1')\n\n        return predictor, clip_model, preprocessor\n\n\n    def score(self, image: Image.Image) -> float:\n        \"\"\"Predicts the CLIP aesthetic score of an image.\"\"\"\n        image = self.preprocessor(image).unsqueeze(0).to('cuda:1')\n\n        with torch.no_grad():\n            image_features = self.clip_model.encode_image(image)\n            # l2 normalize\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n            image_features = image_features.cpu().detach().numpy()\n\n        score = self.predictor(torch.from_numpy(image_features).to('cuda:1').float())\n\n        return score.item() / 10.0  # scale to [0, 1]\n\n\ndef harmonic_mean(a: float, b: float, beta: float = 1.0) -> float:\n    \"\"\"\n    Calculate the harmonic mean of two values, weighted using a beta parameter.\n\n    Args:\n        a: First value (e.g., precision)\n        b: Second value (e.g., recall)\n        beta: Weighting parameter\n\n    Returns:\n        Weighted harmonic mean\n    \"\"\"\n    # Handle zero values to prevent division by zero\n    if a <= 0 or b <= 0:\n        return 0.0\n    return (1 + beta**2) * (a * b) / (beta**2 * a + b)\n\n\ndef svg_to_png(svg_code: str, size: tuple = (384, 384)) -> Image.Image:\n    \"\"\"\n    Converts an SVG string to a PNG image using CairoSVG.\n\n    If the SVG does not define a `viewBox`, it will add one using the provided size.\n\n    Parameters\n    ----------\n    svg_code : str\n        The SVG string to convert.\n    size : tuple[int, int], default=(384, 384)\n        The desired size of the output PNG image (width, height).\n\n    Returns\n    -------\n    PIL.Image.Image\n        The generated PNG image.\n    \"\"\"\n    # Ensure SVG has proper size attributes\n    if 'viewBox' not in svg_code:\n        svg_code = svg_code.replace('<svg', f'<svg viewBox=\"0 0 {size[0]} {size[1]}\"')\n\n    # Convert SVG to PNG\n    png_data = cairosvg.svg2png(bytestring=svg_code.encode('utf-8'))\n    return Image.open(io.BytesIO(png_data)).convert('RGB').resize(size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:51:57.980625Z","iopub.execute_input":"2025-04-27T19:51:57.980980Z","iopub.status.idle":"2025-04-27T19:51:58.856842Z","shell.execute_reply.started":"2025-04-27T19:51:57.980959Z","shell.execute_reply":"2025-04-27T19:51:58.856064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\nglobal_vqa_evaluator = None\nglobal_aesthetic_evaluator = None\n\ndef initialize_evaluators():\n    \"\"\"Initialize the evaluators once and store them in global variables\"\"\"\n    global global_vqa_evaluator, global_aesthetic_evaluator\n    \n    if global_vqa_evaluator is None:\n        print(\"Initializing VQA Evaluator...\")\n        global_vqa_evaluator = VQAEvaluator()\n    \n    if global_aesthetic_evaluator is None:\n        print(\"Initializing Aesthetic Evaluator...\")\n        global_aesthetic_evaluator = AestheticEvaluator()\n    \n    return global_vqa_evaluator, global_aesthetic_evaluator\n\n\ndef evaluate_with_competition_metric(svg, prompt):\n\n    vqa_evaluator, aesthetic_evaluator = initialize_evaluators()\n\n    image = svg_to_png(svg)\n    # Calculate scores\n    vqa_score = vqa_evaluator.score(image, 'SVG illustration of ' + prompt)\n    aesthetic_score = aesthetic_evaluator.score(image)\n    combined_score = harmonic_mean(vqa_score, aesthetic_score, beta=2.0)\n    \n    return {\n        'vqa_score': vqa_score,\n        'aesthetic_score': aesthetic_score,\n        'combined_score': combined_score\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:52:02.825723Z","iopub.execute_input":"2025-04-27T19:52:02.826260Z","iopub.status.idle":"2025-04-27T19:52:02.914721Z","shell.execute_reply.started":"2025-04-27T19:52:02.826238Z","shell.execute_reply":"2025-04-27T19:52:02.914002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Just doing this to prevent model load times from impacting benchmarking of evaluation times\n# Happens on first image evaluation otherwise (no need to export this cell)\ninitialize_evaluators() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:52:04.990573Z","iopub.execute_input":"2025-04-27T19:52:04.991145Z","iopub.status.idle":"2025-04-27T19:52:54.060822Z","shell.execute_reply.started":"2025-04-27T19:52:04.991118Z","shell.execute_reply":"2025-04-27T19:52:54.060033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\n# Ensure GPU is being used and optimize for speed\ndevice = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nimport torch\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\n\n# Load with optimized scheduler and half precision\nstable_diffusion_path = kagglehub.model_download(\"stabilityai/stable-diffusion-v2/pytorch/1/1\")\n\nscheduler = DDIMScheduler.from_pretrained(stable_diffusion_path, subfolder=\"scheduler\")\n\npipe = StableDiffusionPipeline.from_pretrained(\n    stable_diffusion_path,\n    scheduler=scheduler,\n    torch_dtype=torch.float16,  # Use half precision\n    safety_checker=None         # Disable safety checker for speed\n)\n\n# Move to GPU and apply optimizations\npipe.to(device) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:53:24.322833Z","iopub.execute_input":"2025-04-27T19:53:24.323508Z","iopub.status.idle":"2025-04-27T19:53:52.043713Z","shell.execute_reply.started":"2025-04-27T19:53:24.323482Z","shell.execute_reply":"2025-04-27T19:53:52.043095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\ndef compress_hex_color(hex_color):\n    \"\"\"Convert hex color to shortest possible representation\"\"\"\n    r, g, b = int(hex_color[1:3], 16), int(hex_color[3:5], 16), int(hex_color[5:7], 16)\n    if r % 17 == 0 and g % 17 == 0 and b % 17 == 0:\n        return f'#{r//17:x}{g//17:x}{b//17:x}'\n    return hex_color\n\ndef extract_features_by_scale(img_np, num_colors=16):\n    \"\"\"\n    Extract image features hierarchically by scale\n    \n    Args:\n        img_np (np.ndarray): Input image\n        num_colors (int): Number of colors to quantize\n    \n    Returns:\n        list: Hierarchical features sorted by importance\n    \"\"\"\n    # Convert to RGB if needed\n    if len(img_np.shape) == 3 and img_np.shape[2] > 1:\n        img_rgb = img_np\n    else:\n        img_rgb = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n    \n    # Convert to grayscale for processing\n    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n    height, width = gray.shape\n    \n    # Perform color quantization\n    pixels = img_rgb.reshape(-1, 3).astype(np.float32)\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n    _, labels, centers = cv2.kmeans(pixels, num_colors, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n    \n    # Quantized image\n    palette = centers.astype(np.uint8)\n    quantized = palette[labels.flatten()].reshape(img_rgb.shape)\n    \n    # Hierarchical feature extraction\n    hierarchical_features = []\n    \n    # Sort colors by frequency\n    unique_labels, counts = np.unique(labels, return_counts=True)\n    sorted_indices = np.argsort(-counts)\n    sorted_colors = [palette[i] for i in sorted_indices]\n    \n    # Center point for importance calculations\n    center_x, center_y = width/2, height/2\n    \n    for color in sorted_colors:\n        # Create color mask\n        color_mask = cv2.inRange(quantized, color, color)\n        \n        # Find contours\n        contours, _ = cv2.findContours(color_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        # Sort contours by area (largest first)\n        contours = sorted(contours, key=cv2.contourArea, reverse=True)\n        \n        # Convert RGB to compressed hex\n        hex_color = compress_hex_color(f'#{color[0]:02x}{color[1]:02x}{color[2]:02x}')\n        \n        color_features = []\n        for contour in contours:\n            # Skip tiny contours\n            area = cv2.contourArea(contour)\n            if area < 20:\n                continue\n            \n            # Calculate contour center\n            m = cv2.moments(contour)\n            if m[\"m00\"] == 0:\n                continue\n            \n            cx = int(m[\"m10\"] / m[\"m00\"])\n            cy = int(m[\"m01\"] / m[\"m00\"])\n            \n            # Distance from image center (normalized)\n            dist_from_center = np.sqrt(((cx - center_x) / width)**2 + ((cy - center_y) / height)**2)\n            \n            # Simplify contour\n            epsilon = 0.02 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            \n            # Generate points string\n            points = \" \".join([f\"{pt[0][0]:.1f},{pt[0][1]:.1f}\" for pt in approx])\n            \n            # Calculate importance (area, proximity to center, complexity)\n            importance = (\n                area * \n                (1 - dist_from_center) * \n                (1 / (len(approx) + 1))\n            )\n            \n            color_features.append({\n                'points': points,\n                'color': hex_color,\n                'area': area,\n                'importance': importance,\n                'point_count': len(approx),\n                'original_contour': approx  # Store original contour for adaptive simplification\n            })\n        \n        # Sort features by importance within this color\n        color_features.sort(key=lambda x: x['importance'], reverse=True)\n        hierarchical_features.extend(color_features)\n    \n    # Final sorting by overall importance\n    hierarchical_features.sort(key=lambda x: x['importance'], reverse=True)\n    \n    return hierarchical_features\n\ndef simplify_polygon(points_str, simplification_level):\n    \"\"\"\n    Simplify a polygon by reducing coordinate precision or number of points\n    \n    Args:\n        points_str (str): Space-separated \"x,y\" coordinates\n        simplification_level (int): Level of simplification (0-3)\n    \n    Returns:\n        str: Simplified points string\n    \"\"\"\n    if simplification_level == 0:\n        return points_str\n    \n    points = points_str.split()\n    \n    # Level 1: Round to 1 decimal place\n    if simplification_level == 1:\n        return \" \".join([f\"{float(p.split(',')[0]):.1f},{float(p.split(',')[1]):.1f}\" for p in points])\n    \n    # Level 2: Round to integer\n    if simplification_level == 2:\n        return \" \".join([f\"{float(p.split(',')[0]):.0f},{float(p.split(',')[1]):.0f}\" for p in points])\n    \n    # Level 3: Reduce number of points (keep every other point, but ensure at least 3 points)\n    if simplification_level == 3:\n        if len(points) <= 4:\n            # If 4 or fewer points, just round to integer\n            return \" \".join([f\"{float(p.split(',')[0]):.0f},{float(p.split(',')[1]):.0f}\" for p in points])\n        else:\n            # Keep approximately half the points, but maintain at least 3\n            step = min(2, len(points) // 3)\n            reduced_points = [points[i] for i in range(0, len(points), step)]\n            # Ensure we keep at least 3 points and the last point\n            if len(reduced_points) < 3:\n                reduced_points = points[:3]\n            if points[-1] not in reduced_points:\n                reduced_points.append(points[-1])\n            return \" \".join([f\"{float(p.split(',')[0]):.0f},{float(p.split(',')[1]):.0f}\" for p in reduced_points])\n    \n    return points_str\n\ndef bitmap_to_svg_layered(image, max_size_bytes=10000, resize=True, target_size=(384, 384), \n                         adaptive_fill=True, num_colors=None):\n    \"\"\"\n    Convert bitmap to SVG using layered feature extraction with optimized space usage\n    \n    Args:\n        image: Input image (PIL.Image)\n        max_size_bytes (int): Maximum SVG size\n        resize (bool): Whether to resize the image before processing\n        target_size (tuple): Target size for resizing (width, height)\n        adaptive_fill (bool): Whether to adaptively fill available space\n        num_colors (int): Number of colors to quantize, if None uses adaptive selection\n    \n    Returns:\n        str: SVG representation\n    \"\"\"\n    # Adaptive color selection based on image complexity\n    if num_colors is None:\n        # Simple heuristic: more colors for complex images\n        if resize:\n            pixel_count = target_size[0] * target_size[1]\n        else:\n            pixel_count = image.size[0] * image.size[1]\n        \n        if pixel_count < 65536:  # 256x256\n            num_colors = 8\n        elif pixel_count < 262144:  # 512x512\n            num_colors = 12\n        else:\n            num_colors = 16\n    \n    # Resize the image if requested\n    if resize:\n        original_size = image.size\n        image = image.resize(target_size, Image.LANCZOS)\n    else:\n        original_size = image.size\n    \n    # Convert to numpy array\n    img_np = np.array(image)\n    \n    # Get image dimensions\n    height, width = img_np.shape[:2]\n    \n    # Calculate average background color\n    if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n        avg_bg_color = np.mean(img_np, axis=(0,1)).astype(int)\n        bg_hex_color = compress_hex_color(f'#{avg_bg_color[0]:02x}{avg_bg_color[1]:02x}{avg_bg_color[2]:02x}')\n    else:\n        bg_hex_color = '#fff'\n    \n    # Start building SVG\n    # Use original dimensions in viewBox for proper scaling when displayed\n    orig_width, orig_height = original_size\n    svg_header = f'<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{orig_width}\" height=\"{orig_height}\" viewBox=\"0 0 {width} {height}\">\\n'\n    svg_bg = f'<rect width=\"{width}\" height=\"{height}\" fill=\"{bg_hex_color}\"/>\\n'\n    svg_base = svg_header + svg_bg\n    svg_footer = '</svg>'\n    \n    # Calculate base size\n    base_size = len((svg_base + svg_footer).encode('utf-8'))\n    available_bytes = max_size_bytes - base_size\n    \n    # Extract hierarchical features\n    features = extract_features_by_scale(img_np, num_colors=num_colors)\n    \n    # If not using adaptive fill, just add features until we hit the limit\n    if not adaptive_fill:\n        svg = svg_base\n        for feature in features:\n            # Try adding the feature\n            feature_svg = f'<polygon points=\"{feature[\"points\"]}\" fill=\"{feature[\"color\"]}\" />\\n'\n            \n            # Check if adding this feature exceeds size limit\n            if len((svg + feature_svg + svg_footer).encode('utf-8')) > max_size_bytes:\n                break\n            \n            # Add the feature\n            svg += feature_svg\n        \n        # Close SVG\n        svg += svg_footer\n        return svg\n    \n    # For adaptive fill, use binary search to find optimal simplification level\n    \n    # First attempt: calculate size of all features at different simplification levels\n    feature_sizes = []\n    for feature in features:\n        feature_sizes.append({\n            'original': len(f'<polygon points=\"{feature[\"points\"]}\" fill=\"{feature[\"color\"]}\" />\\n'.encode('utf-8')),\n            'level1': len(f'<polygon points=\"{simplify_polygon(feature[\"points\"], 1)}\" fill=\"{feature[\"color\"]}\" />\\n'.encode('utf-8')),\n            'level2': len(f'<polygon points=\"{simplify_polygon(feature[\"points\"], 2)}\" fill=\"{feature[\"color\"]}\" />\\n'.encode('utf-8')),\n            'level3': len(f'<polygon points=\"{simplify_polygon(feature[\"points\"], 3)}\" fill=\"{feature[\"color\"]}\" />\\n'.encode('utf-8'))\n        })\n    \n    # Two-pass approach: first add most important features, then fill remaining space\n    svg = svg_base\n    bytes_used = base_size\n    added_features = set()\n    \n    # Pass 1: Add most important features at original quality\n    for i, feature in enumerate(features):\n        feature_svg = f'<polygon points=\"{feature[\"points\"]}\" fill=\"{feature[\"color\"]}\" />\\n'\n        feature_size = feature_sizes[i]['original']\n        \n        if bytes_used + feature_size <= max_size_bytes:\n            svg += feature_svg\n            bytes_used += feature_size\n            added_features.add(i)\n    \n    # Pass 2: Try to add remaining features with progressive simplification\n    for level in range(1, 4):  # Try simplification levels 1-3\n        for i, feature in enumerate(features):\n            if i in added_features:\n                continue\n                \n            feature_size = feature_sizes[i][f'level{level}']\n            if bytes_used + feature_size <= max_size_bytes:\n                feature_svg = f'<polygon points=\"{simplify_polygon(feature[\"points\"], level)}\" fill=\"{feature[\"color\"]}\" />\\n'\n                svg += feature_svg\n                bytes_used += feature_size\n                added_features.add(i)\n    \n    # Finalize SVG\n    svg += svg_footer\n    \n    # Double check we didn't exceed limit\n    final_size = len(svg.encode('utf-8'))\n    if final_size > max_size_bytes:\n        # If we somehow went over, return basic SVG\n        return f'<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 {width} {height}\"><rect width=\"{width}\" height=\"{height}\" fill=\"{bg_hex_color}\"/></svg>'\n    \n    # Calculate space utilization\n    utilization = (final_size / max_size_bytes) * 100\n    \n    # Return the SVG with efficient space utilization\n    return svg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:54:13.909683Z","iopub.execute_input":"2025-04-27T19:54:13.910264Z","iopub.status.idle":"2025-04-27T19:54:13.934991Z","shell.execute_reply.started":"2025-04-27T19:54:13.910241Z","shell.execute_reply":"2025-04-27T19:54:13.934345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\ndef generate_bitmap(prompt, negative_prompt=\"\", num_inference_steps=20, guidance_scale=15):\n        \n    image = pipe(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=num_inference_steps, \n        guidance_scale=guidance_scale,\n    ).images[0]\n    \n    return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:54:25.576564Z","iopub.execute_input":"2025-04-27T19:54:25.577305Z","iopub.status.idle":"2025-04-27T19:54:25.580975Z","shell.execute_reply.started":"2025-04-27T19:54:25.577277Z","shell.execute_reply":"2025-04-27T19:54:25.580274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\ndef generate_and_convert(prompt, prompt_prefix=\"\", prompt_suffix=\"\", negative_prompt=\"\", num_attempts=3, num_inference_steps=20, guidance_scale=15, verbose=True):\n    \"\"\"\n    Generate image with Stable Diffusion, convert to SVG, and evaluate using competition metric\n    with side-by-side comparison of original and SVG-converted images\n    \"\"\"\n    best_svg = None\n    best_bitmap = None\n    best_similarity = -1\n    \n    # Track total processing time\n    total_start_time = time.time()\n    \n    # Track timing statistics\n    generation_times = []\n    conversion_times = []\n    evaluation_times = []\n    attempt_times = []\n\n    combined_prompt = prompt_prefix + \" \" + prompt + \" \" + prompt_suffix\n        \n    for i in range(num_attempts):\n        attempt_start_time = time.time()\n        if verbose: print(f\"\\n=== Attempt {i+1}/{num_attempts} ===\")\n        \n        # Generate bitmap with Stable Diffusion (using combined_prompt)\n        generation_start = time.time()\n        bitmap = generate_bitmap(combined_prompt, negative_prompt=negative_prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale)\n        generation_end = time.time()\n        generation_time = generation_end - generation_start\n        generation_times.append(generation_time)\n                \n        # Convert to SVG with size limit\n        if verbose: print(f\"Converting to SVG... \", end = \"\")\n        conversion_start = time.time()\n        svg_content = bitmap_to_svg_layered(bitmap)\n        conversion_end = time.time()\n        conversion_time = conversion_end - conversion_start\n        conversion_times.append(conversion_time)\n                \n        # Render SVG to bitmap for evaluation\n        rendered_svg = svg_to_png(svg_content)\n        svg_size = len(svg_content.encode('utf-8'))\n        if verbose: print(f\"SVG size: {svg_size} bytes\")\n\n        if verbose: \n            # Display the images side by side\n            plt.figure(figsize=(12, 6))\n            \n            # Original bitmap\n            plt.subplot(1, 2, 1)\n            plt.imshow(bitmap)\n            plt.title(f\"Original Image {i+1}\")\n            plt.axis('off')\n            \n            # SVG conversion\n            plt.subplot(1, 2, 2)\n            plt.imshow(rendered_svg)\n            plt.title(f\"SVG Conversion {i+1}\")\n            plt.axis('off')\n            \n            plt.tight_layout()\n            plt.show()\n        \n        # Evaluate rendered SVG with competition metric (using just base prompt)\n        evaluation_start = time.time()\n        svg_scores = evaluate_with_competition_metric(svg_content, prompt)\n        evaluation_end = time.time()\n        evaluation_time = evaluation_end - evaluation_start\n        evaluation_times.append(evaluation_time)\n                \n        if verbose:\n            print(f\"SVG VQA Score: {svg_scores['vqa_score']:.4f}\")\n            print(f\"SVG Aesthetic Score: {svg_scores['aesthetic_score']:.4f}\")\n            print(f\"SVG Competition Score: {svg_scores['combined_score']:.4f}\")\n                \n        # Track the best result using competition score\n        if svg_scores['combined_score'] > best_similarity:\n            best_similarity = svg_scores['combined_score']\n            best_svg = svg_content\n            best_bitmap = bitmap\n            if verbose: print(f\"✅ New best result: {svg_scores['combined_score']:.4f}\")\n        else:\n            if verbose: print(f\"❌ Not better than current best: {best_similarity:.4f}\")\n        \n        # Calculate total time for this attempt\n        attempt_end_time = time.time()\n        attempt_time = attempt_end_time - attempt_start_time\n        attempt_times.append(attempt_time)\n        \n        if verbose:\n            print(f\"Image generation time: {generation_time:.2f}s\")\n            print(f\"SVG conversion time: {conversion_time:.2f}s\")\n            print(f\"Image evaluation time: {evaluation_time:.2f}s\")\n            print(f\"Total time for attempt {i+1}: {attempt_time:.2f}s\")\n    \n    # Calculate total processing time\n    total_end_time = time.time()\n    total_time = total_end_time - total_start_time\n    \n    # Print timing summary if verbose\n    if verbose:\n        print(\"\\n=== Timing Summary ===\")\n        print(f\"Average image generation time: {sum(generation_times)/len(generation_times):.2f}s\")\n        print(f\"Average SVG conversion time: {sum(conversion_times)/len(conversion_times):.2f}s\")\n        print(f\"Average image evaluation time: {sum(evaluation_times)/len(evaluation_times):.2f}s\")\n        print(f\"Average time per attempt: {sum(attempt_times)/len(attempt_times):.2f}s\")\n        print(f\"Total processing time ({num_attempts} attempts): {total_time:.2f}s\")\n        print(f\"Best score achieved: {best_similarity:.4f}\")\n                    \n    return best_svg, best_similarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:54:48.690637Z","iopub.execute_input":"2025-04-27T19:54:48.691218Z","iopub.status.idle":"2025-04-27T19:54:48.701399Z","shell.execute_reply.started":"2025-04-27T19:54:48.691198Z","shell.execute_reply":"2025-04-27T19:54:48.700822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Note: These parameters below are just for testing - modify parameters in the model definition for submission\n\nprompt_prefix = \"Simple, classic image of\"\n        \nprompt = \"a lighthouse overlooking the ocean\"\n\nprompt_suffix = \"with flat color blocks, beautiful, minimal details, solid colors only\"\n\nnegative_prompt = \"lines, framing, hatching, background, textures, patterns, details, outlines\"\n\nbest_svg, best_score = generate_and_convert(\n    prompt, \n    prompt_prefix=prompt_prefix, \n    prompt_suffix=prompt_suffix, \n    negative_prompt=negative_prompt, \n    num_inference_steps=25, \n    guidance_scale=20, \n    num_attempts=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:54:58.568312Z","iopub.execute_input":"2025-04-27T19:54:58.568918Z","iopub.status.idle":"2025-04-27T19:56:14.342643Z","shell.execute_reply.started":"2025-04-27T19:54:58.568896Z","shell.execute_reply":"2025-04-27T19:56:14.341982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\nclass Model:\n    def __init__(self):\n        '''Optional constructor, performs any setup logic, model instantiation, etc.'''\n        \n        # Set number of image attempts per prompt for competition here...\n\n        self.num_attempts_per_prompt = 3\n        self.num_inference_steps = 25\n        self.guidance_scale = 20\n\n        self.prompt_prefix = \"Simple, classic image of\"\n        self.prompt_suffix = \"with flat color blocks, beautiful, minimal details, solid colors only\"\n        self.negative_prompt = \"lines, framing, hatching, background, textures, patterns, details, outlines\"\n\n        self.last_score = None\n            \n        pass\n\n    def predict(self, prompt: str) -> str:\n        '''Generates SVG which produces an image described by the prompt.\n\n        Args:\n            prompt (str): A prompt describing an image\n        Returns:\n            String of valid SVG code.\n        '''\n        \n        best_svg, best_score = generate_and_convert(\n            prompt,\n            prompt_prefix=self.prompt_prefix,\n            prompt_suffix=self.prompt_suffix,            \n            negative_prompt=self.negative_prompt,\n            num_attempts=self.num_attempts_per_prompt,\n            num_inference_steps=self.num_inference_steps,\n            guidance_scale=self.guidance_scale,\n            verbose=False\n        )\n\n        self.last_score = best_score\n        \n        return best_svg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T19:56:50.106367Z","iopub.execute_input":"2025-04-27T19:56:50.106639Z","iopub.status.idle":"2025-04-27T19:56:50.112214Z","shell.execute_reply.started":"2025-04-27T19:56:50.106620Z","shell.execute_reply":"2025-04-27T19:56:50.111461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read the CSV file\ndf = pd.read_csv('/kaggle/input/drawing-with-llms/train.csv')\n\n# uncomment to test on just a few\n#df = df.head(3)\n\n# Initialize the model\nmodel = Model()\n\n# Create arrays to store scores and timing data\nscores = []\ngeneration_times = []\n\nfor i, row in enumerate(df.iterrows()):\n    description = row[1]['description']\n    \n    # Start timing\n    start_time = time.time()\n    \n    # Generate image from description\n    svg = model.predict(description)\n    rendered_img = svg_to_png(svg)\n    \n    # End timing\n    end_time = time.time()\n    generation_time = end_time - start_time\n    generation_times.append(generation_time)\n    \n    # Get the score\n    score = model.last_score\n    scores.append(score)\n        \n    # Display the image being processed\n    plt.figure(figsize=(10, 8))\n    plt.imshow(rendered_img)\n    plt.title(f\"Best image for: {description}\\nScore: {score:.2f}\")\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \n    # Print progress, current average score, and timing info\n    current_avg_score = np.mean(scores)\n    current_avg_time = np.mean(generation_times)\n    \n    print(f\"Processed {i+1}/{len(df)} prompts\")\n    print(f\"Current average score: {current_avg_score:.2f}\")\n    print(f\"Time for this prompt: {generation_time:.2f}s\")\n    print(f\"Current average generation time: {current_avg_time:.2f}s\")\n    \n# When all done, calculate final statistics\navg_score = np.mean(scores)\navg_generation_time = np.mean(generation_times)\ntotal_time_taken = sum(generation_times)\n\n# Calculate projections for 500 images\nprojected_time_500_images = 500 * avg_generation_time\nprojected_hours = projected_time_500_images / 3600\n\nprint(\"\\n=== SUMMARY ===\")\nprint(f\"Prompts processed: {len(df)}\")\nprint(f\"Final average score: {avg_score:.2f}\")\nprint(f\"Average generation time per prompt: {avg_generation_time:.2f} seconds\")\nprint(f\"Total time elapsed: {timedelta(seconds=total_time_taken)}\")\nprint(f\"Projected time for 500 prompts: {projected_hours:.2f} hours ({timedelta(seconds=projected_time_500_images)})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:00:22.304631Z","iopub.execute_input":"2025-04-27T20:00:22.304956Z","iopub.status.idle":"2025-04-27T20:12:46.991247Z","shell.execute_reply.started":"2025-04-27T20:00:22.304936Z","shell.execute_reply":"2025-04-27T20:12:46.990551Z"}},"outputs":[],"execution_count":null}]}